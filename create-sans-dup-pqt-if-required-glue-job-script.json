{
	"jobConfig": {
		"name": "create-sans-dup-pqt-if-required",
		"description": "",
		"role": "arn:aws:iam::xxxxxxxxxxx:role/service-role/AWSGlueServiceRole",
		"command": "pythonshell",
		"version": "1.0",
		"runtime": null,
		"workerType": null,
		"numberOfWorkers": null,
		"maxCapacity": 0.0625,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "create-sans-dup-pqt-if-required.py",
		"scriptLocation": "s3://aws-glue-assets-xxxxxxxxxxxx-us-east-1/scripts/",
		"language": "python-3.9",
		"spark": false,
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-08-31T02:58:28.189Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-xxxxxxxxxxxx-us-east-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"observabilityMetrics": false,
		"pythonShellPrebuiltLibraryOption": "analytics",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nimport boto3\n\nclient = boto3.client('athena')\ns3_client = boto3.client('s3')\ns3_resource = boto3.resource('s3')\n\nSOURCE_TABLE_NAME = 'apple_stock_aggregate_data_parquet_tbl'\nNEW_TABLE_NAME = 'apple_stock_aggregate_data_sans_dup_parquet_tbl'\nNEW_TABLE_S3_BUCKET = 's3://prod-sans-dup-pqt-stock-aggregate-data-1453891/'\nMY_DATABASE = 'stockaggregatedata'\nQUERY_RESULTS_S3_BUCKET = 's3://athena-query-results-first-de-project-aug-2024-1639045/'\n\n# Extract the bucket name and prefix from the S3 path\nbucket_name = 'prod-sans-dup-pqt-stock-aggregate-data-1453891'\nprefix = ''  # No prefix in the path after the bucket name\n\n# Delete any existing files in the S3 bucket\ndef delete_existing_s3_folder(bucket_name, prefix):\n    bucket = s3_resource.Bucket(bucket_name)\n    bucket.objects.filter(Prefix=prefix).delete()\n\ndelete_existing_s3_folder(bucket_name, prefix)\n\n# Check for duplicates\nqueryStart = client.start_query_execution(\n    QueryString = f\"\"\"\n    CREATE TABLE {NEW_TABLE_NAME} WITH\n    (external_location='{NEW_TABLE_S3_BUCKET}',\n    format='PARQUET',\n    write_compression='SNAPPY',\n    partitioned_by = ARRAY['day_hour_partition'])\n    AS\n\n    SELECT\n    normal_timestamp,\n    TradVol,\n    VolWtAvg,\n    OpenPrice,\n    ClosePrice,\n    HighestPrice,\n    LowestPrice,\n    day_hour_partition\nFROM (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (\n            PARTITION BY normal_timestamp, day_hour_partition, TradVol, VolWtAvg, OpenPrice, ClosePrice, HighestPrice, LowestPrice\n            ORDER BY normal_timestamp DESC\n        ) AS rn\n        FROM \"{MY_DATABASE}\".\"{SOURCE_TABLE_NAME}\"\n    ) AS ranked\n    WHERE rn = 1\n    ;\n    \"\"\",\n\n    QueryExecutionContext = {\n        'Database': f'{MY_DATABASE}'\n    }, \n    ResultConfiguration = { 'OutputLocation': f'{QUERY_RESULTS_S3_BUCKET}'}\n)\n\n# list of responses\nresp = [\"FAILED\", \"SUCCEEDED\", \"CANCELLED\"]\n\n# get the response\nresponse = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\n\n# wait until query finishes\nwhile response[\"QueryExecution\"][\"Status\"][\"State\"] not in resp:\n    response = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\n    \n# if it fails, exit and give the Athena error message in the logs\nif response[\"QueryExecution\"][\"Status\"][\"State\"] == 'FAILED':\n    sys.exit(response[\"QueryExecution\"][\"Status\"][\"StateChangeReason\"])"
}