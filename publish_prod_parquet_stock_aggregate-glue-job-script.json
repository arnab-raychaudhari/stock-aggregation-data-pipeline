{
	"jobConfig": {
		"name": "publish_prod_parquet_stock_aggregate",
		"description": "",
		"role": "arn:aws:iam::xxxxxxxxxxxx:role/service-role/AWSGlueServiceRole",
		"command": "pythonshell",
		"version": "1.0",
		"runtime": null,
		"workerType": null,
		"numberOfWorkers": null,
		"maxCapacity": 0.0625,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "publish_prod_parquet_stock_aggregate.py",
		"scriptLocation": "s3://aws-glue-assets-xxxxxxxxxxxx-us-east-1/scripts/",
		"language": "python-3.9",
		"spark": false,
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-08-28T19:15:30.809Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-xxxxxxxxxxxx-us-east-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"observabilityMetrics": false,
		"pythonShellPrebuiltLibraryOption": "analytics",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nimport boto3\nfrom datetime import datetime\n\nQUERY_RESULTS_BUCKET = 's3://athena-query-results-first-de-project-aug-2024-1639045/'\nMY_DATABASE = 'stockaggregatedata'\nSOURCE_PARQUET_TABLE_NAME = 'apple_stock_aggregate_data_sans_dup_parquet_tbl'\nNEW_PROD_PARQUET_TABLE_NAME = 'prod_apple_stock_aggregate_data_parquet_tbl'\nNEW_PROD_PARQUET_TABLE_S3_BUCKET = 's3://prod-pqt-stock-aggregate-data-1453891/'\n\n# create a string with the current UTC datetime\n# convert all special characters to underscores\n# this will be used in the table name and in the bucket path in S3 where the table is stored\nDATETIME_NOW_INT_STR = str(datetime.now()).replace('-', '_').replace(' ', '_').replace(':', '_').replace('.', '_')\n\nclient = boto3.client('athena')\n\n# Refresh the table\nqueryStart = client.start_query_execution(\n    QueryString = f\"\"\"\n    CREATE TABLE {NEW_PROD_PARQUET_TABLE_NAME}_{DATETIME_NOW_INT_STR} WITH\n    (external_location='{NEW_PROD_PARQUET_TABLE_S3_BUCKET}/{DATETIME_NOW_INT_STR}/',\n    format='PARQUET',\n    write_compression='SNAPPY',\n    partitioned_by = ARRAY['day_hour_partition'])\n    AS\n\n    SELECT\n        *\n    FROM \"{MY_DATABASE}\".\"{SOURCE_PARQUET_TABLE_NAME}\"\n\n    ;\n    \"\"\",\n    QueryExecutionContext = {\n        'Database': f'{MY_DATABASE}'\n    }, \n    ResultConfiguration = { 'OutputLocation': f'{QUERY_RESULTS_BUCKET}'}\n)\n\n# list of responses\nresp = [\"FAILED\", \"SUCCEEDED\", \"CANCELLED\"]\n\n# get the response\nresponse = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\n\n# wait until query finishes\nwhile response[\"QueryExecution\"][\"Status\"][\"State\"] not in resp:\n    response = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\n    \n# if it fails, exit and give the Athena error message in the logs\nif response[\"QueryExecution\"][\"Status\"][\"State\"] == 'FAILED':\n    sys.exit(response[\"QueryExecution\"][\"Status\"][\"StateChangeReason\"])\n\n"
}